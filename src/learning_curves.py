import pandas as pd
import numpy as np
import json
import os
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error
import yaml

# ========== –ó–ê–ì–†–£–ó–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í ==========
with open('params.yaml', 'r', encoding='utf-8') as f:
    params = yaml.safe_load(f)
    model_params = params['linear_models']

# ========== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ==========
print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
X_train = pd.read_csv('data/processed/X_train.csv')
y_train = pd.read_csv('data/processed/y_train.csv').squeeze()
X_val = pd.read_csv('data/processed/X_val.csv')
y_val = pd.read_csv('data/processed/y_val.csv').squeeze()
X_test = pd.read_csv('data/processed/X_test.csv')
y_test = pd.read_csv('data/processed/y_test.csv').squeeze()

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

os.makedirs('reports/learning_curves', exist_ok=True)
os.makedirs('metrics/learning_curves', exist_ok=True)

# ========== 1. –ö–†–ò–í–ê–Ø –û–ë–£–ß–ï–ù–ò–Ø ==========
def plot_learning_curve(model, model_name, X, y):
    """–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è: –æ—à–∏–±–∫–∞ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏"""
    print(f"üìà Learning curve –¥–ª—è {model_name}...")
    
    train_sizes = np.linspace(0.1, 1.0, 10) * len(X)
    train_scores = []
    val_scores = []
    
    for size in train_sizes:
        size = int(size)
        X_subset = X[:size]
        y_subset = y[:size]
        
        model_copy = model.__class__(**model.get_params())
        model_copy.fit(X_subset, y_subset)
        
        train_pred = model_copy.predict(X_subset)
        val_pred = model_copy.predict(X_val)
        
        train_scores.append(mean_squared_error(y_subset, train_pred))
        val_scores.append(mean_squared_error(y_val, val_pred))
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_scores, 'o-', label='Train MSE', color='blue', linewidth=2)
    plt.plot(train_sizes, val_scores, 'o-', label='Validation MSE', color='red', linewidth=2)
    plt.xlabel('–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏')
    plt.ylabel('MSE')
    plt.title(f'–ö—Ä–∏–≤–∞—è –æ–±—É—á–µ–Ω–∏—è - {model_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f'reports/learning_curves/{model_name}_learning_curve.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    data = {
        'train_sizes': train_sizes.tolist(),
        'train_mse': train_scores,
        'val_mse': val_scores
    }
    with open(f'metrics/learning_curves/{model_name}_learning.json', 'w') as f:
        json.dump(data, f, indent=2)
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

# ========== 2. –ê–ù–ê–õ–ò–ó –û–°–¢–ê–¢–ö–û–í ==========
def plot_residuals(model, model_name, X_train, y_train, X_test, y_test):
    """–ê–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –æ—à–∏–±–∫–∏"""
    print(f"üìä Residuals plot –¥–ª—è {model_name}...")
    
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    train_residuals = y_train - train_pred
    test_residuals = y_test - test_pred
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # –û—Å—Ç–∞—Ç–∫–∏ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (train)
    axes[0, 0].scatter(train_pred, train_residuals, alpha=0.5, s=10)
    axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)
    axes[0, 0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (train)')
    axes[0, 0].set_ylabel('–û—Å—Ç–∞—Ç–∫–∏')
    axes[0, 0].set_title('–û—Å—Ç–∞—Ç–∫–∏ –Ω–∞ train')
    axes[0, 0].grid(True, alpha=0.3)
    
    # –û—Å—Ç–∞—Ç–∫–∏ vs –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (test)
    axes[0, 1].scatter(test_pred, test_residuals, alpha=0.5, s=10)
    axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)
    axes[0, 1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (test)')
    axes[0, 1].set_ylabel('–û—Å—Ç–∞—Ç–∫–∏')
    axes[0, 1].set_title('–û—Å—Ç–∞—Ç–∫–∏ –Ω–∞ test')
    axes[0, 1].grid(True, alpha=0.3)
    
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ (train)
    axes[1, 0].hist(train_residuals, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=1)
    axes[1, 0].set_xlabel('–û—Å—Ç–∞—Ç–∫–∏')
    axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ (train)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Q-Q plot
    from scipy import stats
    stats.probplot(train_residuals, dist="norm", plot=axes[1, 1])
    axes[1, 1].set_title('Q-Q plot –æ—Å—Ç–∞—Ç–∫–æ–≤')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.suptitle(f'–ê–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤ - {model_name}', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'reports/learning_curves/{model_name}_residuals.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

# ========== 3. –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í ==========
def plot_feature_importance(model, model_name, feature_names):
    """–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)"""
    print(f"üìä Feature importance –¥–ª—è {model_name}...")
    
    if not hasattr(model, 'coef_'):
        print(f"   ‚ö†Ô∏è –ù–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤")
        return
    
    coef = model.coef_
    importance = np.abs(coef)
    
    indices = np.argsort(importance)[::-1]
    sorted_features = [feature_names[i] for i in indices]
    sorted_importance = importance[indices]
    sorted_coef = coef[indices]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
    axes[0].barh(range(len(sorted_importance)), sorted_importance, color='steelblue')
    axes[0].set_yticks(range(len(sorted_importance)))
    axes[0].set_yticklabels(sorted_features)
    axes[0].set_xlabel('–ê–±—Å–æ–ª—é—Ç–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å |–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç|')
    axes[0].set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ –º–æ–¥—É–ª—é)')
    axes[0].invert_yaxis()
    axes[0].grid(True, alpha=0.3, axis='x')
    
    # –ó–Ω–∞—á–µ–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
    colors = ['green' if c > 0 else 'red' for c in sorted_coef]
    axes[1].barh(range(len(sorted_coef)), sorted_coef, color=colors)
    axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    axes[1].set_yticks(range(len(sorted_coef)))
    axes[1].set_yticklabels(sorted_features)
    axes[1].set_xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞')
    axes[1].set_title('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏')
    axes[1].invert_yaxis()
    axes[1].grid(True, alpha=0.3, axis='x')
    
    plt.suptitle(f'Feature importance - {model_name}', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'reports/learning_curves/{model_name}_importance.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    data = {
        'features': sorted_features,
        'coefficients': sorted_coef.tolist(),
        'importance': sorted_importance.tolist()
    }
    with open(f'metrics/learning_curves/{model_name}_importance.json', 'w') as f:
        json.dump(data, f, indent=2)
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ")

# ========== –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ==========
def get_model_params(model_dict):
    """–£–±–∏—Ä–∞–µ—Ç 'enabled' –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    return {k: v for k, v in model_dict.items() if k != 'enabled'}

models = {
    'ridge': Ridge(**get_model_params(model_params['ridge'])),
    'lasso': Lasso(**get_model_params(model_params['lasso'])),
    'elastic': ElasticNet(**get_model_params(model_params['elastic']))
}

for model_name, model in models.items():
    if not model_params[model_name]['enabled']:
        continue
        
    print(f"\n{'='*50}")
    print(f"üìä –ê–Ω–∞–ª–∏–∑ –¥–ª—è {model_name}")
    print(f"{'='*50}")
    
    model.fit(X_train, y_train)
    plot_learning_curve(model, model_name, X_train, y_train)
    plot_residuals(model, model_name, X_train, y_train, X_test, y_test)
    plot_feature_importance(model, model_name, X_train.columns.tolist())

print("\n‚úÖ –í—Å–µ –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã!")