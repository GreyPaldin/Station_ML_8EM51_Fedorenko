import pandas as pd
import json
import shutil
import os

# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
SOURCE_DATASET = "data/raw/Summary_of_Weather.csv"
STATIONS_DATASET = "data/raw/Weather_Station_Locations.csv"
WORKING_COPY_PATH = "data/processed/working_copy.csv"
COLUMNS_TO_DELETE = ["WindGustSpd","DR","SPD",
"MAX", "MIN", "MEA","SND","FT","FB","FTI","ITH","PGT",
"TSHDSBRSGF","SD3","RHX","RHN","RVG","WTE","PoorWeather",
"LAT","LON","MinTemp","MaxTemp","NAME","STATE/COUNTRY ID","Date","PRCP","SNF","DA","YR"] #"MinTemp","MaxTemp"
WEATHER_STATION_ID = "STA"
STATIONS_STATION_ID = "WBAN"
PROBLEM_VALUE = 9999
PROBLEM_COLUMN = "ELEV"
STATION_ID_IN_STATIONS = "WBAN"
# ======================================================

def create_copy(src, dst):
    os.makedirs(os.path.dirname(dst), exist_ok=True)
    shutil.copy2(src, dst)
    return pd.read_csv(dst), dst

def merge_with_stations(weather_df, stations_df, weather_id_col, stations_id_col):
    stations_df_renamed = stations_df.rename(columns={stations_id_col: weather_id_col})
    return pd.merge(weather_df, stations_df_renamed, on=weather_id_col, how='left', suffixes=('', '_station'))

def find_problem_stations(stations_df, problem_col, problem_val, station_id_col):
    if problem_col not in stations_df.columns:
        print(f"–ö–æ–ª–æ–Ω–∫–∞ {problem_col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ stations_df")
        print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(stations_df.columns)}")
        return []
    problem_stations = stations_df[stations_df[problem_col] == problem_val]
    return problem_stations[station_id_col].tolist()

def clean_problem_stations(stations_df, problem_col, problem_val):
    """–£–¥–∞–ª–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç–∞–Ω—Ü–∏–∏ –∏–∑ stations_df –ø–µ—Ä–µ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º"""
    if problem_col not in stations_df.columns:
        return stations_df
    
    initial_count = len(stations_df)
    stations_clean = stations_df[stations_df[problem_col] != problem_val].copy()
    removed = initial_count - len(stations_clean)
    
    if removed > 0:
        print(f"–£–¥–∞–ª–µ–Ω–æ –∏–∑ stations_df: {removed} —Å—Ç–∞–Ω—Ü–∏–π —Å {problem_col}={problem_val}")
    
    return stations_clean

def delete_columns(df, columns):
    existing = [c for c in columns if c in df.columns]
    return df.drop(columns=existing) if existing else df

def delete_duplicates(df):
    initial = len(df)
    df_clean = df.drop_duplicates()
    removed = initial - len(df_clean)
    if removed > 0:
        print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed}")
    return df_clean

def convert_snowfall_to_float_simple(df):
    """
    –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Snowfall –≤ float.
    –ù–µ-—Ü–∏—Ñ—Ä—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è NaN.
    """
    if 'Snowfall' in df.columns:
        print(f"üîÑ Snowfall: {df['Snowfall'].dtype} -> float")
        df['Snowfall'] = pd.to_numeric(df['Snowfall'], errors='coerce')
        nulls_added = df['Snowfall'].isna().sum() - df['Snowfall'].isna().sum()
        if nulls_added > 0:
            print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ NaN: {nulls_added} —Å—Ç—Ä–æ–∫")
    return df

def delete_null_rows(df):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Å Null –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º"""
    initial_rows = len(df)
    
    # –°—á–∏—Ç–∞–µ–º Null –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
    null_counts = df.isnull().sum()
    null_counts = null_counts[null_counts > 0].sort_values(ascending=False)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    if len(null_counts) > 0:
        print("Null –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
        for col, count in null_counts.items():
            percent = (count / initial_rows) * 100
            print(f"  {col}: {count} —Å—Ç—Ä–æ–∫ ({percent:.1f}%)")
    else:
        print("Null –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ—Ç")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ª—é–±—ã–º Null
    df_cleaned = df.dropna()
    removed = initial_rows - len(df_cleaned)
    
    if removed > 0:
        print(f"–£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å Null: {removed} ({removed/initial_rows*100:.1f}%)")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å: {len(df_cleaned)} —Å—Ç—Ä–æ–∫")
    else:
        print("Null —Å—Ç—Ä–æ–∫ –Ω–µ—Ç")
    
    return df_cleaned

def print_analysis(df, label=""):
    """–í—ã–≤–æ–¥ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª—å"""
    if label:
        print(f"\n{label}")
    print(f"–°—Ç—Ä–æ–∫: {len(df):,}")
    print(f"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    print(f"–î—É–ø–ª–∏–∫–∞—Ç–æ–≤: {df.duplicated().sum():,}")
    

def delite_T():
    df['Precip'] = df['Precip'].replace('T', '0')

# ==================== –ò–°–ü–û–õ–ù–ï–ù–ò–ï ====================
if __name__ == "__main__":
    print("="*50)
    print("–ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò")
    print("="*50)
    
    # 1. –ö–æ–ø–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    df, copy_path = create_copy(SOURCE_DATASET, WORKING_COPY_PATH)
    print_analysis(df, "1. –ö–æ–ø–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    
    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞–Ω—Ü–∏–π
    if os.path.exists(STATIONS_DATASET):
        stations_df = pd.read_csv(STATIONS_DATASET)
        print(f"\n2. –î–∞–Ω–Ω—ã–µ —Å—Ç–∞–Ω—Ü–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {stations_df.shape}")
        
        # 3. –ù–ê–ô–¢–ò –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç–∞–Ω—Ü–∏–∏
        problem_ids = find_problem_stations(stations_df, PROBLEM_COLUMN, PROBLEM_VALUE, STATION_ID_IN_STATIONS)
        print(f"   –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π: {len(problem_ids)}")
        
        # 4. –£–î–ê–õ–ò–¢–¨ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç–∞–Ω—Ü–∏–∏ –ò–ó stations_df
        stations_df_clean = clean_problem_stations(stations_df, PROBLEM_COLUMN, PROBLEM_VALUE)
        print(f"   –°—Ç–∞–Ω—Ü–∏–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {stations_df_clean.shape}")
        
        # 5. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å –û–ß–ò–©–ï–ù–ù–´–ú–ò —Å—Ç–∞–Ω—Ü–∏—è–º–∏
        df = merge_with_stations(df, stations_df_clean, WEATHER_STATION_ID, STATIONS_STATION_ID)
        print_analysis(df, "\n5. –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ —Å—Ç–∞–Ω—Ü–∏—è–º–∏:")
    
    # 6. –£–¥–∞–ª–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏
    if COLUMNS_TO_DELETE:
        initial_cols = len(df.columns)
        df = delete_columns(df, COLUMNS_TO_DELETE)
        print(f"\n6. –£–¥–∞–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {initial_cols - len(df.columns)}")
        print(f"   –û—Å—Ç–∞–ª–æ—Å—å –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    
    # 7. –î—É–±–ª–∏–∫–∞—Ç—ã
    df = delete_duplicates(df)
    print_analysis(df, "\n7. –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
   
    # 7.5. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å Snowfall –≤ float
    print("\n7.5. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Snowfall –≤ float:")
    df = convert_snowfall_to_float_simple(df)
    
    # 8. Null —Å—Ç—Ä–æ–∫–∏
    print("\n8. –£–¥–∞–ª–µ–Ω–∏–µ Null —Å—Ç—Ä–æ–∫:")
    df = delete_null_rows(df)
    
    # 8.1 –£–¥–∞–ª–µ–Ω–∏–µ T –≤ Precip
    delite_T()
    
    # 9. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    df.to_csv(copy_path, index=False)
    print(f"\n9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {os.path.basename(copy_path)}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {df.shape}")
    
    # 9.5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è DVC
    metrics = {
        'original_rows': int(pd.read_csv(SOURCE_DATASET).shape[0]),
        'final_rows': len(df),
        'rows_removed': int(pd.read_csv(SOURCE_DATASET).shape[0] - len(df)),
        'original_columns': int(pd.read_csv(SOURCE_DATASET).shape[1]),
        'final_columns': len(df.columns),
        'columns_removed': len(COLUMNS_TO_DELETE),
        'stations_removed': len(find_problem_stations(pd.read_csv(STATIONS_DATASET), PROBLEM_COLUMN, PROBLEM_VALUE, STATION_ID_IN_STATIONS)) if os.path.exists(STATIONS_DATASET) else 0,
        'duplicates_removed': int(pd.read_csv(SOURCE_DATASET).shape[0] - len(df))  # –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
    }
    
    os.makedirs('metrics', exist_ok=True)
    with open('metrics/prepare_metrics.json', 'w') as f:
        import json
        json.dump(metrics, f, indent=2)
    
    # 10. –ò—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    print("\n" + "="*50)
    print("–ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó")
    print("="*50)
    print(f"–°—Ç—Ä–æ–∫: {len(df):,}")
    print(f"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")